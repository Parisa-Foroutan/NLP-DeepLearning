{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Scientific_paper_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-7.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parisa-Foroutan/Deep-Learning/blob/main/Scientific_paper_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SCEvxuJz7lE"
      },
      "source": [
        "# Kaggle Competition\n",
        "\n",
        "For this task, I have tried to fine-tune the pretrained models from the Hugging Face library. I have tested models such as BertForSequenceClassification,RobertaForSequenceClassification, XLNetForSequenceClassification, AlbertForSequenceClassification, and  DebertaForSequenceClassification. Among these, I got the highest accuracy with utilizing microsoft/deberta-base tokenizer and the pretrained model and the next best model was bert-based-uncased. I have also tried using large models such as Ber-large or DeBerta-large, but it was not possible to run due to cuda memory limitations.\n",
        "\n",
        "I have used one sequence of concatinating the title and the abstract and tried to cleen the data a little more to get rid of some rare tokens. For the validation, I have used 10% of the training dataset.\n",
        "\n",
        "Moreover, I have tried various hyperparameter settings by changing the MAX_LEN (max of sequence length), batch_size, learning rate, value for gradient clipping, hidden_dropout_prob, and attention_probs_dropout_prob."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRuLdswJz7lc"
      },
      "source": [
        "# Install Hugging face library\n",
        "!pip install transformers\n",
        "# for using XLNet\n",
        "!pip install SentencePiece \n",
        "\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEgAK7IllxrE"
      },
      "source": [
        "## Import general libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHn4EfIzz7la"
      },
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maBdd4yPz7le"
      },
      "source": [
        "## Loading Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBJf7pl0p7-I",
        "outputId": "b909af25-6726-4422-e1b8-f0722ff54a2d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "CikHTSQzqdKN",
        "scrolled": true,
        "outputId": "597a2414-8a5b-4eb4-cc5f-31a8d95c547c"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/HW2_DL_document_classification/my_train.csv', header=0, encoding='utf-8')\n",
        "# df = pd.read_csv('train.csv', header=0, encoding='utf-8')\n",
        "# to use both title and abstract:\n",
        "df['text'] = df.iloc[:, 3] + \" \" + df.iloc[:, 4]\n",
        "df = df.drop(df.columns[[1, 2, 3, 4]], axis=1)\n",
        "\n",
        "print(df.shape[0])\n",
        "df.sample(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23550</th>\n",
              "      <td>16</td>\n",
              "      <td>centernet keypoint triplets for object detecti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35905</th>\n",
              "      <td>14</td>\n",
              "      <td>symbolic versus numerical computation and visu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45182</th>\n",
              "      <td>6</td>\n",
              "      <td>living without a mobile phone an autoethnograp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                               text\n",
              "23550     16  centernet keypoint triplets for object detecti...\n",
              "35905     14  symbolic versus numerical computation and visu...\n",
              "45182      6  living without a mobile phone an autoethnograp..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsa9mxLQuLOf"
      },
      "source": [
        "# prepare the list of sentences and labels\n",
        "sentences = df.text.values\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNqjTtxbuLOf"
      },
      "source": [
        "# function to clean-up the data\n",
        "def clean_sequence(sent):\n",
        "    # Removing the @\n",
        "    sent = re.sub(r\"@[A-Za-z0-9]+\", ' ', sent)\n",
        "    # Removing the $\n",
        "    sent = re.sub(r\"$\\?[A-Za-z0-9]+\", '$', sent)\n",
        "    # Removing the URL links\n",
        "    sent = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', sent)\n",
        "    # Keeping only letters\n",
        "    sent = re.sub(r\"[^a-zA-Z0-9.()!?'%$]\", ' ', sent)\n",
        "    # Removing additional whitespaces\n",
        "    sent = re.sub(r\" +\", ' ', sent)\n",
        "    return sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frfjv-U1uLOh"
      },
      "source": [
        "sentences = [clean_sequence(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Z5x7LGz7li"
      },
      "source": [
        "## Tokenization & Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlwGS9uuz7lj"
      },
      "source": [
        "from transformers import BertTokenizer, RobertaTokenizer, XLNetTokenizer, AlbertTokenizer, DebertaTokenizer\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base', do_lower_case=True)\n",
        "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True) \n",
        "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yPbWKWnz7lk"
      },
      "source": [
        "### Sentence Length & Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhqzZltNz7ll",
        "outputId": "46e9415a-e527-48b7-c908-60cefdbaa5f9"
      },
      "source": [
        "# Tokenize all of the sentences\n",
        "input_ids = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "print('Sentence: ', sentences[0])\n",
        "print('Token IDs: ', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sentence:  evasion attacks against machine learning at test time In security sensitive applications the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent well motivated attack scenario an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work we present a simple but effective gradient based approach that can be exploited to systematically assess the security of several widely used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.\n",
            "Token IDs:  [1, 3623, 27720, 1912, 136, 3563, 2239, 23, 1296, 86, 11, 573, 5685, 2975, 5, 1282, 9, 3563, 2239, 7971, 15, 10, 10675, 23371, 9, 49, 5910, 7, 37930, 27774, 414, 4, 11, 65, 28870, 157, 7958, 908, 5665, 41, 32714, 189, 2120, 7, 25866, 10, 6400, 467, 23, 1296, 86, 30, 7015, 30830, 908, 7931, 4, 11, 42, 173, 52, 1455, 10, 2007, 53, 2375, 43141, 716, 1548, 14, 64, 28, 19251, 7, 28210, 7118, 5, 573, 9, 484, 3924, 341, 20257, 16964, 136, 23397, 1912, 4, 511, 10, 682, 1850, 7208, 13, 573, 10437, 52, 34233, 908, 12593, 14, 8483, 430, 810, 1389, 13, 5, 1380, 24072, 30, 2284, 5, 12082, 18, 2655, 9, 5, 467, 8, 69, 1460, 7, 21922, 908, 7931, 4, 42, 2029, 5, 1380, 24072, 6004, 10, 357, 2170, 9, 5, 1380, 24072, 819, 223, 23397, 1912, 8, 2386, 123, 7, 3008, 10, 55, 3978, 1421, 4230, 36, 368, 43797, 2749, 322, 52, 10516, 84, 1548, 15, 5, 4249, 573, 3685, 9, 16886, 12673, 11, 46559, 6773, 8, 311, 14, 215, 1743, 64, 28, 2773, 7630, 7560, 4, 52, 67, 15923, 103, 3231, 37939, 2528, 30, 84, 1966, 4, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqXqXKouz7lm"
      },
      "source": [
        "### Padding & Truncating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBEzcjAEz7ln",
        "outputId": "d90d5885-d841-4301-91d7-7f047912d644"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  1009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ULhWoxfz7lo",
        "outputId": "5286b5f8-baa5-43b1-9872-dbafbde8f743"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "# the maximum possible length in the pretrained models is 512, however we have longer sequences. So we have to truncate them.\n",
        "# In some cases, even using 512 is not possible due to cuda limitations.\n",
        "MAX_LEN = 450\n",
        "# Pad or truncate all sentences to # of MAX_LEN\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print(f'\\nPadding token: {tokenizer.pad_token} , ID: {tokenizer.pad_token_id}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding token: [PAD] , ID: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOm9c6Faz7lo"
      },
      "source": [
        "**Attention Masks:**\n",
        "\n",
        "The attention mask will distinguish the real tokens from padding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snzHZvG7z7lp"
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISKKMTWzz7lr"
      },
      "source": [
        "### split data to training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSc6OMi8z7ls"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=12, test_size=0.10)\n",
        "\n",
        "train_masks, valid_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=12, test_size=0.10)\n",
        "\n",
        "# Convert all inputs and labels into torch tensors\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "valid_inputs = torch.tensor(valid_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "valid_masks = torch.tensor(valid_masks)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "valid_labels = torch.tensor(valid_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMZkyZ0Nz7lu"
      },
      "source": [
        "# Creating DataLoader objects\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 6\n",
        "\n",
        "# DataLoader for training\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# DataLoader for validation\n",
        "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNfgjeNcz7lv"
      },
      "source": [
        "##  Train the Text Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3thBGySz7ly"
      },
      "source": [
        "### Pretrained Models from Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNAdVw0Updlb"
      },
      "source": [
        "I have used pretrained models (such as BertForSequenceClassification) from the Hugging Face library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcyOPsYYz7l1",
        "outputId": "cdd5effa-1cbd-4680-c5e3-81225751f6e5"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, RobertaForSequenceClassification, \\\n",
        "                         RobertaConfig, XLNetForSequenceClassification, XLNetConfig, AlbertForSequenceClassification,\\\n",
        "                         DebertaForSequenceClassification\n",
        "\n",
        "\n",
        "# config = RobertaConfig.from_pretrained('roberta-base', num_labels = 20)\n",
        "# config.hidden_dropout_prob  = 0.15\n",
        "# config.attention_probs_dropout_prob = 0.15\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",  num_labels = 20)\n",
        "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\",  num_labels = 20)\n",
        "# model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\",  num_labels = 20)\n",
        "# model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config = config)\n",
        "# model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", config = config)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'config']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DebertaForSequenceClassification(\n",
              "  (deberta): DebertaModel(\n",
              "    (embeddings): DebertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "      (LayerNorm): DebertaLayerNorm()\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (1): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (2): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (3): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (4): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (5): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (6): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (7): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (8): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (9): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (10): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "        (11): DebertaLayer(\n",
              "          (attention): DebertaAttention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(1024, 768)\n",
              "    )\n",
              "  )\n",
              "  (pooler): ContextPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
              "  (dropout): StableDropout()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oATZnyFGz7l3"
      },
      "source": [
        "###  Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmMzla-Wz7l4"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "\n",
        "# AdamW is a class from the huggingface library ('Weight Decay fix\") \n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "# Total number of training steps\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LPljCmIz7l4"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quAf0-x8z7l5"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def accuracy_score(preds, labels):\n",
        "    predicted = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(predicted == labels_flat) / len(labels_flat)\n",
        "\n",
        "# function for formatting elapsed times\n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    report time in hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmIo_WdGCeKw"
      },
      "source": [
        "I couldn't use large models such as Ber-large or DeBerta-large because of the following GPU limitations:\n",
        "```\n",
        "bert-large-uncased:  RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.76 GiB total capacity; 13.59 GiB already allocated; 125.75 MiB free; 13.61 GiB reserved in total by PyTorch)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHmAbG1Az7l6"
      },
      "source": [
        "# average loss after each epoch\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "\n",
        "    # checkpoint = torch.load('/content/drive/MyDrive/Datasets/HW2_DL_document_classification/checkpoint/model.pt')\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    # epoch = checkpoint['epoch'] + 1\n",
        "    # loss = checkpoint['loss']\n",
        "\n",
        "        ## Training ##\n",
        "    print(f'====== Epoch {epoch + 1} / {epochs} ======')\n",
        "    print('Training...')\n",
        " \n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # batch: (input ids, attention masks, labels)  \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()        \n",
        "        \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        # Accumulate the training loss over all of the batches \n",
        "        total_loss += loss.item()\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, '/content/drive/MyDrive/Datasets/HW2_DL_document_classification/checkpoint/model.pt')\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = (total_loss / len(train_dataloader)).detach().cpu().item()   \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "\n",
        "    print(f\"  Average training loss: {avg_train_loss}\")\n",
        "    print(f\"  Training time for this epoch: {format_time(time.time() - t0)}\")\n",
        "        \n",
        "    ## Validation ##\n",
        "\n",
        "    print(\"\\nValidation...\")\n",
        "    t0 = time.time()\n",
        "    valid_loss, valid_accuracy, best_valid_acc = 0, 0, 0\n",
        "    nb_valid_steps, nb_valid_examples = 0, 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # batch accuracy\n",
        "        tmp_valid_accuracy = accuracy_score(logits, label_ids)  \n",
        "        # update total accuracy.\n",
        "        valid_accuracy += tmp_valid_accuracy\n",
        "        nb_valid_steps += 1\n",
        "\n",
        "    # Report validation accuracy.\n",
        "    print(f\"  Validation Accuracy: {valid_accuracy/nb_valid_steps}\")\n",
        "    print(f\"  Validation time: {format_time(time.time() - t0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntz5X9XLdrlC"
      },
      "source": [
        "fig = plt.figure(figsize=(6,3))\n",
        "plt.plot(range(epochs), loss_values, label='loss')\n",
        "plt.title('Training loss of the Model')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIds3v1-z7l8"
      },
      "source": [
        "## Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpkZwESJz7l9"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vbFtsHLDDUt",
        "outputId": "cdc91bc5-fc19-4780-f346-d20bca4c762c"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/HW2_DL_document_classification/my_test.csv', header=0, encoding='utf-8')\n",
        "# df = pd.read_csv('my_test.csv', header=0, encoding='utf-8')\n",
        "df['text'] = df.iloc[:, 2] + \" \" + df.iloc[:, 3]\n",
        "print('Number of papers in test set: {:,}\\n'.format(df.shape[0]))\n",
        "print(df.sample(3))\n",
        "sentences = df.text.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of papers in test set: 13,718\n",
            "\n",
            "       node id  ...                                               text\n",
            "4691    148592  ...  modular tracking framework a unified approach ...\n",
            "11752   164862  ...  cartesian effect categories are freyd categori...\n",
            "7277    154568  ...  baby step giant step algorithms for the symmet...\n",
            "\n",
            "[3 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDZ1tjwJ-cBg"
      },
      "source": [
        "sentences = [clean_sequence(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqhtNkZhz7l9"
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for sent in sentences:\n",
        "    encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "# Pad input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "attention_masks = []\n",
        "# Create attention_masks\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "\n",
        "batch_size = 1 \n",
        "# DataLoader\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoZUrW8wz7l9"
      },
      "source": [
        "### Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rid5b8ccz7l-",
        "outputId": "3670f34c-81ae-41c7-dcc9-ba9752ab5dfd"
      },
      "source": [
        "# Prediction on test set\n",
        "print(f'Predicting labels for {len(prediction_inputs)} test sentences...')\n",
        "\n",
        "# model.load_state_dict(best_state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for test_batch in prediction_dataloader:\n",
        "  # Add test_batch to GPU\n",
        "  test_batch = tuple(t.to(device) for t in test_batch)\n",
        "  b_input_ids, b_input_mask = test_batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  predictions.append(logits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 13718 test sentences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwqOLcHoT5-T"
      },
      "source": [
        "### Saving my predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06px3PRz1K1R"
      },
      "source": [
        "my_labels = pd.DataFrame(np.argmax(predictions, axis=2))\n",
        "my_labels['node id'] = df['node id']\n",
        "my_labels.columns = ['label', 'node id']\n",
        "my_labels = my_labels[['node id', 'label']]\n",
        "my_labels.to_csv('sorted_predictions_bert_clean_450.csv')\n",
        "my_labels.to_csv('/content/drive/MyDrive/Datasets/HW2_DL_document_classification/sorted_predictions_bert_clean_450.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}